setting:
  global:
    tensor_para_size: 2
    pipeline_para_size: 1
    enable_lora_adapter: false
  batch_scheduler:
    waiting_timeout_in_ms: 600000
    max_waiting_queue_len: 100
    max_step_tokens: 8192
    max_batch_size: 32
    max_token_len: 2048
  block_manager:
    block_token_num: 16
    reserved_device_memory_ratio: 0.01
    lora_deivce_memory_ratio: 0.0
    lora_host_memory_factor: 10.0
    block_device_memory_ratio: -1.0
    block_host_memory_factor: 0.0
  tokenization:
    add_special_tokens: true
    skip_special_tokens: true
model_spec:
  base_model:
    model_dir: /model/llama-hf/7B/
# 仅在跑模型性能压测的时候有效
model_performance_runner_config:
  # 输入的配置
  input_config:
    # decode请求的个数
    single_token_request_num: 2
    # 每条decode请求已经拥有的kv cache的长度
    single_token_request_cached_token_num: 32
    # context请求的个数
    multi_token_request_num: 2
    # contex请求已经存在的kv cache的长度
    multi_token_cached_token_num: 30
    # 每条context请求的长度
    multi_token_request_token_num: 32
  runner_config:
    # warmup阶段压测的轮数
    warmup_rounds: 10
    # 压测的轮数
    rounds: 100


